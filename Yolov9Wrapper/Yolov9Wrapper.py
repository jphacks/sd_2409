"""
# Yolov9Wrapper.py
Yolov9ã‚’ä½¿ã†ã‚„ã™ãã™ã‚‹ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹

## ä½¿ã„æ–¹
- ã¯ã˜ã‚ã«
    - yolov9ã®ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã¦ã€requirements.txtã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹

```py
from Yolov9Wrapper import Yolov9

# ---ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
model = Yolov9(weights="./20240625.pt")

# ---1æšã®ç”»åƒã‚’æ¨è«–ã™ã‚‹
result, times = model.predict_image("./20240625_5690.jpg")
# çµæœã‚’è¡¨ç¤ºã™ã‚‹
boxes = result["boxes"]
for box in boxes:
    label = box["label"]
    conf = box["confidence"]
    xyxy = box["xyxy"]
    print(label, conf, xyxy)
    
cv2.imshow("result", image)
cv2.waitKey(0)
cv2.destroyAllWindows()

# ---Webã‚«ãƒ¡ãƒ©ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’æ¨è«–ã™ã‚‹
# ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã‚’ç”¨æ„ã™ã‚‹
def on_detect(result:Yolov9Result):
    boxes = result["boxes"]
    for box in boxes:
        print(box["label"], box["confidence"], box["xyxy"])

model.predict_stream(0, on_detect=on_detect)


```

## ã®ã“ã‚Šã‚„ã‚‹ã“ã¨
- ğŸ™†å€¤ã®è¿”å´
    - bboxã®ç”»åƒ
    - çµæœã®dict
        - ãƒ©ãƒ™ãƒ«
        - ç¢ºä¿¡åº¦
        - bboxã®åº§æ¨™
- ç´°ã‹ã„ã¨ã“ã‚ã®èª¿æ•´
    - ç”»åƒã®ä¿å­˜å…ˆã®æŒ‡å®š
    - ä¸è¦ãªã‚³ãƒ¼ãƒ‰ã®å‰Šé™¤

## æ›´æ–°å±¥æ­´
- 20241012
    - np.ndarrayã‚’ç›´æ¥æ¨è«–ã§ãã‚‹ã‚ˆã†ã«ã—ãŸ
    - Yolov9Annotatorã‚’è¿½åŠ ã—ãŸ
    - Yolov9Resultã«ã€annotatorã‚’è¿½åŠ ã—ãŸ
        - ã‚ã¨ã‹ã‚‰bboxæç”»ãŒã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚
- 20240728
    - è¤‡æ•°æšã®ç”»åƒã‚’æ¨è«–ã§ãã‚‹ã‚ˆã†ã«ã—ãŸ
    - LabelMeDataã‚’è¿½åŠ ã—ãŸ
        - Yolov9Resultã‚’LabelMeDataã«å¤‰æ›ã™ã‚‹é–¢æ•°ã§ã€å¤‰æ›ã§ãã‚‹ã‚ˆã†ã«ã—ãŸ
- 20240725
    - å€¤ã®è¿”å´ã‚’ã™ã‚‹ã‚ˆã†ã«ã—ãŸ
    - `detect_stream()`ã‚’è¿½åŠ ã—ãŸ
- 20240713
    - ãŒã‚ŠãŒã‚Šå‰Šã‚Šå–ã£ãŸ
    - ã€Œã®ã“ã‚Šã‚„ã‚‹ã“ã¨ã€ãŒæ®‹ã£ã¦ã„ã¾ã™â€¦â€¦
    
"""

from pathlib import Path
import platform
from typing import Tuple

import numpy as np
import torch
import sys
import base64
import json
from tqdm import tqdm

print(f"\033[36m[YoloV9Wrapper] importing yolov9...\033[0m")
sys.path.append(r"./yolov9")
from yolov9.models.common import DetectMultiBackend
from yolov9.utils.dataloaders import LoadImages, LoadStreams
from yolov9.utils.general import (
    Profile,
    check_img_size,
    check_imshow,
    cv2,
    non_max_suppression,
    scale_boxes,
    xyxy2xywh,
)
from yolov9.utils.plots import Annotator, colors, save_one_box
from yolov9.utils.torch_utils import select_device, smart_inference_mode

print(f"\033[36m[YoloV9Wrapper] imported yolov9!\033[0m")

from typing import TypedDict

from Yolov9Wrapper.LabelMeData import LabelMeData, Shape


class Yolov9Annotator:
    def __init__(
        self,
        image: np.ndarray,
        line_width: int = None,
        font_size: float = None,
        font: str = "Arial.ttf",
        pil: bool = False,
        names: dict[int, str] = {0: "label_name"},
    ) -> None:
        """Yolov9ã®çµæœã‚’æç”»ã™ã‚‹ãŸã‚ã®ã‚¯ãƒ©ã‚¹

        Args:
            image (np.ndarray): æç”»ã™ã‚‹ç”»åƒ
            line_width (int, optional): bboxã®ç·šã®å¤ªã•. Defaults to None.
            font_size (float, optional): ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º. Defaults to None.
            font (str, optional): ãƒ•ã‚©ãƒ³ãƒˆ. Defaults to "Arial.ttf".
            pil (bool, optional): ä¸æ˜ã€‚ Defaults to False.
            names (list[str], optional): ãƒ©ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆ. Defaults to None.
        """
        self.annotator = Annotator(
            image,
            line_width=line_width,
            font_size=font_size,
            font=font,
            pil=pil,
            example=str(names),
        )
        self.names = names
        self.image = image

    def box_label(
        self,
        box: tuple[float, float, float, float],
        label="",
        color=(128, 128, 128),
        txt_color=(255, 255, 255),
    ):
        """ç”»åƒã«bboxã‚’æç”»ã™ã‚‹

        Args:
            box (tuple[float, float, float, float]): bboxã®åº§æ¨™(0-1)
            label (str, optional): ãƒ©ãƒ™ãƒ«. Defaults to "".
            color (tuple, optional): bboxã®è‰². Defaults to (128, 128, 128).
            txt_color (tuple, optional): ãƒ†ã‚­ã‚¹ãƒˆã®è‰². Defaults to (255, 255, 255).
        """
        self.annotator.box_label(box, label, color, txt_color)

    def result(self) -> np.ndarray:
        """æç”»ã—ãŸç”»åƒã‚’è¿”ã™

        Returns:
            np.ndarray: æç”»ã—ãŸç”»åƒ
        """
        return self.annotator.result()
    
    def get_color(self, label: str) -> tuple[int, int, int]:
        """ãƒ©ãƒ™ãƒ«ã«å¯¾å¿œã™ã‚‹è‰²ã‚’è¿”ã™

        Args:
            label (str): ãƒ©ãƒ™ãƒ«

        Returns:
            tuple[int, int, int]: è‰²
        """
        color_index = 0
        for index, name in self.names.items():
            if name == label:
                color_index = index
                break
        return colors(index, True)


class Yolov9ResultBox(TypedDict):
    """Yolov9ã®æ¨è«–çµæœã®bboxã‚’ä¿å­˜ã™ã‚‹ã‚¯ãƒ©ã‚¹

    example:
    ä¸‹ã®ã‚ˆã†ãªå½¢ã§ä¿å­˜ã•ã‚Œã‚‹
    ```json
    {
        "label": str,  # ãƒ©ãƒ™ãƒ«
        "confidence": float,  # ç¢ºä¿¡åº¦
        "xyxy": Tuple[float, float, float, float],  # bboxã®åº§æ¨™(0-1)
    }
    ```

    """

    label: str  # ãƒ©ãƒ™ãƒ«
    confidence: float  # ç¢ºä¿¡åº¦
    xyxy: Tuple[float, float, float, float]  # bboxã®åº§æ¨™(0-1)

    def __repr__(self):
        return f"Yolov9ResultBox(label={self.label}, confidence={self.confidence}, bbox={self.xyxy})"


class Yolov9Result(TypedDict):
    """Yolov9ã®æ¨è«–çµæœã‚’ä¿å­˜ã™ã‚‹ã‚¯ãƒ©ã‚¹

    example:
    ä¸‹ã®ã‚ˆã†ãªå½¢ã§ä¿å­˜ã•ã‚Œã‚‹
    ```json
    {
        "image": np.ndarray,  # bboxã®ç”»åƒ
        "path": str|None,  # ç”»åƒã®ãƒ‘ã‚¹ã€‚numpyã®å ´åˆã¯None
        "boxes": [
            {
                "label": str,  # ãƒ©ãƒ™ãƒ«
                "confidence": float,  # ç¢ºä¿¡åº¦
                "xyxy": Tuple[float, float, float, float],  # bboxã®åº§æ¨™(0-1)
            }
        ],
        "annotator": Yolov9Annotator  # annotator
    }
    ```

    """

    image: np.ndarray  # bboxã®ç”»åƒ
    path: str  # ç”»åƒã®ãƒ‘ã‚¹
    boxes: list[Yolov9ResultBox]  # bboxã®ãƒªã‚¹ãƒˆ
    annotator: Yolov9Annotator  # annotator

    def __repr__(self):
        return f"Yolov9Result(image={self.image}, path={self.path}, boxes={self.boxes})"


def convert_to_labelme(yolov9_result: Yolov9Result) -> LabelMeData:
    """Yolov9Resultã‚’LabelMeDataã«å¤‰æ›ã™ã‚‹"""
    image_height, image_width = yolov9_result["image"].shape[:2]
    shapes: list[Shape] = []
    for box in yolov9_result["boxes"]:
        label = box["label"]
        xyxy = box["xyxy"]
        points = [
            [xyxy[0] * image_width, xyxy[1] * image_height],
            [xyxy[2] * image_width, xyxy[3] * image_height],
        ]
        shape = Shape(
            label=label,
            points=points,
            group_id=None,
            description="",
            shape_type="rectangle",
            flags={},
            mask=None,
        )
        shapes.append(shape)

    with open(yolov9_result["path"], "rb") as f:
        base64_image_data = base64.b64encode(f.read()).decode("utf-8")

    labelme_data = LabelMeData(
        version="5.5.0",
        flags={},
        shapes=shapes,
        imagePath=yolov9_result["path"],
        imageData=base64_image_data,
        imageHeight=image_height,
        imageWidth=image_width,
    )

    return labelme_data


class Yolov9:
    def __init__(
        self,
        weights: str,  # model path
        device="",  # cuda device, i.e. 0 or 0,1,2,3 or cpu
        data="./yolov9/data/coco.yaml",  # dataset.yaml path
        dnn=False,  # use OpenCV DNN for ONNX inference
        half=False,  # FP16 half-precision inference
    ) -> None:
        self.weights = weights
        self.device = device
        self.data = data
        self.dnn = dnn
        self.half = half
        # ---ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        self.device = select_device(device)
        self.model = DetectMultiBackend(
            self.weights,
            device=self.device,
            dnn=self.dnn,
            data=self.data,
            fp16=self.half,
        )

    @smart_inference_mode()
    def predict_image(
        self,
        images: str | list[str] | np.ndarray | list[np.ndarray],  # image path
        save_dir: str | None = None,  # save image path
        imgsz=(640, 640),  # inference size (height, width)
        conf_thres=0.25,  # confidence threshold
        iou_thres=0.45,  # NMS IOU threshold
        max_det=1000,  # maximum detections per image
        classes=None,  # filter by class: --class 0, or --class 0 2 3
        agnostic_nms=False,  # class-agnostic NMS
        augment=False,  # augmented inference
        line_thickness=3,  # bounding box thickness (pixels)
        hide_labels=False,  # hide labels
        hide_conf=False,  # hide confidences
    ) -> Tuple[list[Yolov9Result], Tuple[float, float, float]]:
        """1æšã®ç”»åƒã‚’æ¨è«–ã™ã‚‹

        Args(ä¸»ãªã‚‚ã®):
            image (str | list[str]): ç”»åƒã®ãƒ‘ã‚¹
            save_dir (str | None): ä¿å­˜å…ˆã®ãƒ‘ã‚¹

        Returns:
            Tuple[Yolov9Result, Tuple[float, float, float]]: æ¨è«–çµæœã¨å‡¦ç†æ™‚é–“
        """
        # ---ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        stride, names, pt = self.model.stride, self.model.names, self.model.pt

        # ---ç”»åƒã®èª­ã¿è¾¼ã¿
        imgsz = check_img_size(imgsz, s=stride)  # check image size
        dataset = LoadImages(images, img_size=imgsz, stride=stride, auto=pt)

        # ---æ¨è«–ã®å®Ÿè¡Œ
        self.model.warmup(imgsz=(1, 3, *imgsz))

        # Profileã®dtã¯ã€æ™‚é–“ã‚’è¨ˆæ¸¬ã™ã‚‹ãŸã‚ã®ã‚„ã¤ã€‚
        dt = (Profile(), Profile(), Profile())

        results: list[Yolov9Result] = []
        for path, im, im0s, _, _ in tqdm(dataset, desc="Detecting objects"):
            # print("\033[31m", "="*20, "\033[0m")
            # ---ç”»åƒã®å‰å‡¦ç†
            with dt[0]:
                im = torch.from_numpy(im).to(self.model.device)
                im = im.half() if self.model.fp16 else im.float()  # uint8 to fp16/32
                im /= 255  # 0 - 255 to 0.0 - 1.0
                if len(im.shape) == 3:
                    im = im[None]  # expand for batch dim

            # ---å®Ÿéš›ã®æ¨è«–
            with dt[1]:
                pred = self.model(im, augment=augment)

            # ---NMSã€‚æ¨è«–ã•ã‚ŒãŸbboxã‚’æ•´ç†ã™ã‚‹å‡¦ç†ã‚‰ã—ã„ã€‚ç°¡å˜ã«è¨€ã†ã¨ã€é‡è¤‡ã—ã¦ã„ã‚‹bboxã‚’å‰Šé™¤ã™ã‚‹å‡¦ç†ã€‚
            with dt[2]:
                pred = non_max_suppression(
                    pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det
                )

            # ---çµæœã®è¡¨ç¤º
            # det = pred[0]
            result: Yolov9Result = Yolov9Result()  # æ¨è«–çµæœã‚’ä¿å­˜ã™ã‚‹ã‚¯ãƒ©ã‚¹
            for det in pred:  # predã€len(pred)=1ç¢ºå®šã˜ã‚ƒã­ï¼Ÿ
                # print("\033[32m", "="*20, "\033[0m")
                p, im0, frame = path, im0s.copy(), getattr(dataset, "frame", 0)

                if type(p) == str:
                    p = Path(p)  # to Path
                elif isinstance(p, np.ndarray):
                    p = ""

                gn = torch.tensor(im0.shape)[
                    [1, 0, 1, 0]
                ]  # å¹…ã€é«˜ã•ã‚’0-1ã«ã™ã‚‹ãŸã‚ã®å€¤

                # ---annotatorã®åˆæœŸåŒ–
                # annotator = Annotator(
                #     im0, line_width=line_thickness, example=str(names)
                # )
                annotator = Yolov9Annotator(im0, line_width=line_thickness, names=names)
                im0_copy=im0.copy()
                annotator_new=Yolov9Annotator(im0_copy, line_width=line_thickness, names=names)

                box_results: list[Yolov9ResultBox] = []  # å„bboxã®çµæœã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
                if len(det):
                    # Rescale boxes from img_size to im0 size
                    det[:, :4] = scale_boxes(
                        im.shape[2:], det[:, :4], im0.shape
                    ).round()

                    # ---å„æ¤œå‡ºbboxã«å¯¾ã—ã¦å‡¦ç†ã™ã‚‹
                    for *xyxy, conf, cls in reversed(det):
                        x_sorted = sorted([xyxy[0], xyxy[2]])
                        y_sorted = sorted([xyxy[1], xyxy[3]])
                        new_xyxy = [x_sorted[0], y_sorted[0], x_sorted[1], y_sorted[1]]
                        xywh = (
                            (xyxy2xywh(torch.tensor(new_xyxy).view(1, 4)) / gn)
                            .view(-1)
                            .tolist()
                        )  # normalized xywh
                        normalized_xyxy = (
                            (torch.tensor(new_xyxy).view(1, 4) / gn).view(-1).tolist()
                        )
                        # print(f"{names[int(cls)]} {conf:.2f} {xywh}")

                        # ---bboxã®æç”»
                        c = int(cls)
                        label = (
                            None
                            if hide_labels
                            else (names[c] if hide_conf else f"{names[c]} {conf:.2f}")
                        )
                        color=colors(c, True)
                        annotator.box_label(new_xyxy, label, color=color)

                        # ---bboxã®çµæœã‚’è¿½åŠ 
                        box_results.append(
                            {
                                "label": names[int(cls)],
                                "confidence": float(conf),
                                "xyxy": normalized_xyxy,
                            }
                        )
                        # ---1ã¤ã®bboxã‚’ä¿å­˜ã™ã‚‹å‡¦ç†ã£ã½ã„
                        # save_one_box(new_xyxy, imc, file=save_dir / 'crops' /
                        #              names[c] / f'{p.stem}.jpg', BGR=True)

                    # Save results (image with detections)
                    if save_dir:
                        save_path = f"{save_dir}/{p.stem}.jpg"
                        cv2.imwrite(save_path, im0)
                im0 = annotator.result()

                # ---result(1ç”»åƒã®çµæœ)ã‚’ç”¨æ„
                result["image"] = im0
                result["path"] = str(p) if type(p) == Path else None
                result["boxes"] = box_results
                result["annotator"] = annotator_new  # è¿½è¨˜ç”¨ã«annotatorã‚‚è¿”ã™

                # ---results(å…¨ç”»åƒã®çµæœ)ã«è¿½åŠ 
                results.append(result)

        # ---å‡¦ç†æ™‚é–“ã®è¨ˆæ¸¬
        t = tuple(x.t * 1e3 for x in dt)  # speeds per image
        # LOGGER.info(
        #     f"Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}"
        #     % t
        # )
        return results, t

    @smart_inference_mode()
    def predict_stream(
        self,
        video_index: int,
        on_detect: callable = None,
        imgsz=(640, 640),  # inference size (height, width)
        conf_thres=0.25,  # confidence threshold
        iou_thres=0.45,  # NMS IOU threshold
        max_det=1000,  # maximum detections per image
        classes=None,  # filter by class: --class 0, or --class 0 2 3
        agnostic_nms=False,  # class-agnostic NMS
        augment=False,  # augmented inference
        line_thickness=3,  # bounding box thickness (pixels)
        hide_labels=False,  # hide labels
        hide_conf=False,  # hide confidences
        vid_stride=1,  # video frame-rate stride
    ):
        # ---ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        stride, names, pt = self.model.stride, self.model.names, self.model.pt

        # ---ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®èª­ã¿è¾¼ã¿
        view_img = check_imshow(warn=True)
        dataset = LoadStreams(
            str(video_index),
            img_size=imgsz,
            stride=stride,
            auto=pt,
            vid_stride=vid_stride,
        )
        bs = len(dataset)

        # ---æ¨è«–ã®å®Ÿè¡Œ
        self.model.warmup(
            imgsz=(1 if pt or self.model.triton else bs, 3, *imgsz)
        )  # warmup

        seen, windows, dt = 0, [], (Profile(), Profile(), Profile())

        for path, im, im0s, _, _ in dataset:
            # ---ç”»åƒã®å‰å‡¦ç†
            with dt[0]:
                im = torch.from_numpy(im).to(self.model.device)
                im = im.half() if self.model.fp16 else im.float()  # uint8 to fp16/32
                im /= 255  # 0 - 255 to 0.0 - 1.0
                if len(im.shape) == 3:
                    im = im[None]  # expand for batch dim

            # ---å®Ÿéš›ã®æ¨è«–
            with dt[1]:
                pred = self.model(im, augment=augment)

            # ---NMSã€‚æ¨è«–ã•ã‚ŒãŸbboxã‚’æ•´ç†ã™ã‚‹å‡¦ç†ã‚‰ã—ã„ã€‚ç°¡å˜ã«è¨€ã†ã¨ã€é‡è¤‡ã—ã¦ã„ã‚‹bboxã‚’å‰Šé™¤ã™ã‚‹å‡¦ç†ã€‚
            with dt[2]:
                pred = non_max_suppression(
                    pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det
                )

            # ---å„bboxçµæœã®è¡¨ç¤º
            for i, det in enumerate(pred):
                result: Yolov9Result = Yolov9Result()
                seen += 1
                p, im0, _ = path[i], im0s[i].copy(), dataset.count

                p = Path(p)  # to Path
                gn = torch.tensor(im0.shape)[
                    [1, 0, 1, 0]
                ]  # å¹…ã€é«˜ã•ã‚’0-1ã«ã™ã‚‹ãŸã‚ã®å€¤
                annotator = Annotator(
                    im0, line_width=line_thickness, example=str(names)
                )
                box_results: list[Yolov9ResultBox] = []  # å„bboxã®çµæœã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
                if len(det):
                    # Rescale boxes from img_size to im0 size
                    det[:, :4] = scale_boxes(
                        im.shape[2:], det[:, :4], im0.shape
                    ).round()

                    # ---å„æ¤œå‡ºbboxã«å¯¾ã—ã¦å‡¦ç†ã™ã‚‹
                    for *xyxy, conf, cls in reversed(det):
                        x_sorted = sorted([xyxy[0], xyxy[2]])
                        y_sorted = sorted([xyxy[1], xyxy[3]])
                        new_xyxy = [x_sorted[0], y_sorted[0], x_sorted[1], y_sorted[1]]
                        xywh = (
                            (xyxy2xywh(torch.tensor(new_xyxy).view(1, 4)) / gn)
                            .view(-1)
                            .tolist()
                        )  # normalized xywh
                        normalized_xyxy = (
                            (torch.tensor(new_xyxy).view(1, 4) / gn).view(-1).tolist()
                        )
                        # print(f"{names[int(cls)]} {conf:.2f} {xywh}")

                        # ---bboxã®æç”»
                        c = int(cls)
                        label = (
                            None
                            if hide_labels
                            else (names[c] if hide_conf else f"{names[c]} {conf:.2f}")
                        )
                        annotator.box_label(new_xyxy, label, color=colors(c, True))

                        # ---bboxã®çµæœã‚’è¿½åŠ 
                        box_results.append(
                            {
                                "label": names[int(cls)],
                                "confidence": float(conf),
                                "xyxy": normalized_xyxy,
                            }
                        )

                im0 = annotator.result()
                # ---æ˜ åƒã®è¡¨ç¤º
                if view_img:
                    if platform.system() == "Linux" and p not in windows:
                        windows.append(p)
                        cv2.namedWindow(
                            str(p), cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO
                        )  # allow window resize (Linux)
                        cv2.resizeWindow(str(p), im0.shape[1], im0.shape[0])
                    cv2.imshow(str(p), im0)
                    cv2.waitKey(1)  # 1 millisecond

                result["image"] = im0
                result["path"] = ""
                result["boxes"] = box_results

                if on_detect is not None and len(box_results) > 0:
                    on_detect(result)

            # ---å„ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‡¦ç†æ™‚é–“ã®è¡¨ç¤º
            t = dt[1].dt * 1e3  # ms
            print(t)

        # ---å‡¦ç†æ™‚é–“ã®è¨ˆæ¸¬
        t = tuple(x.t * 1e3 for x in dt)  # speeds per image
        # LOGGER.info(
        #     f"Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}"
        #     % t
        # )
        return result, t
